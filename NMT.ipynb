{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS"
      ],
      "metadata": {
        "id": "V7mGmaYQIGFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlrZW5tVN5Dp",
        "outputId": "52ae07c3-8fc7-4102-f654-339656efd930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum \n",
        "\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "NEifyXRzIIOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelSettings(Enum):\n",
        "  SEQUENCE_LENGTH = 20\n",
        "  BATCH_SIZE = 64\n",
        "  SHUFFLE_SEED = 2048\n",
        "  PREFETCH = 16\n",
        "  EMBEDDING_DIMENSION = 256\n",
        "  LATENT_DIMENSION = 2048\n",
        "  NUMBER_HEADS = 8\n",
        "  EPOCHS = 1\n",
        "  DROPOUT_RATE = 0.5\n",
        "  MAXIMUM_GENERATION_LENGTH = 20\n",
        "  ACTIVATION = \"relu\"\n",
        "  OPTIMISER = \"rmsprop\"\n",
        "  LOSS = \"sparse_categorical_crossentropy\"\n",
        "  METRIC = \"accuracy\"\n",
        "  TOKENISER = \"t5-small\"\n",
        "  STYLE_QUESTION = \"<|style:question|>\"\n",
        "  STYLE_STATEMENT = \"<|style:statement|>\"\n",
        "  SENTIMENT_POSITIVE = \"<|sentiment:positive|>\"\n",
        "  SENTIMENT_NEGATIVE = \"<|sentiment:negative|>\"\n",
        "  SENTIMENT_NEUTRAL = \"<|sentiment:neutral|>\"\n",
        "\n",
        "MAP_SPECIAL_TOKEN = dict(\n",
        "    positive=ModelSettings.SENTIMENT_POSITIVE.value,\n",
        "    negative=ModelSettings.SENTIMENT_NEGATIVE.value,\n",
        "    neutral=ModelSettings.SENTIMENT_NEUTRAL.value,\n",
        "    statement=ModelSettings.STYLE_STATEMENT.value,\n",
        "    question=ModelSettings.STYLE_QUESTION.value\n",
        ")\n",
        "TOKENISER = AutoTokenizer.from_pretrained(ModelSettings.TOKENISER.value)\n",
        "TOKENISER.add_special_tokens(\n",
        "    dict(\n",
        "        bos_token=\"<|startoftext|>\",\n",
        "        additional_special_tokens=[\n",
        "            ModelSettings.STYLE_QUESTION.value,\n",
        "            ModelSettings.STYLE_STATEMENT.value,\n",
        "            ModelSettings.SENTIMENT_POSITIVE.value,\n",
        "            ModelSettings.SENTIMENT_NEGATIVE.value,\n",
        "            ModelSettings.SENTIMENT_NEUTRAL.value,\n",
        "        ]\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtmyXes9IJR-",
        "outputId": "9b8776d8-b9fe-44f6-87dd-fd729639c533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA"
      ],
      "metadata": {
        "id": "6mAwyd3SILVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs = [\n",
        "(('positive','statement',['police','crime']), 'The police attended a crime'),\n",
        "(('negative','statement',['hope','verdict']), 'Her hopes were dashed when she heard the verdict.'),\n",
        "(('neutral','statement',['relax']),'Please relax.'),\n",
        "(('positive','question',['japan','food']),'I eat Japanese food.'),\n",
        "(('negative','question',['mary','present','daughter']),\"Mary bought a present for her friend's daughter.\")\n",
        "]"
      ],
      "metadata": {
        "id": "mp3if8PdGLXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Dict, List\n"
      ],
      "metadata": {
        "id": "1MfgjZj2Nor-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_sentence(sentence:str) -> str:\n",
        "  return f\"{TOKENISER.bos_token} {sentence} {TOKENISER.eos_token}\" \n",
        "\n",
        "def format_condition(style:str,sentiment:str,keywords:List[str]) -> str:\n",
        "  return f\"{MAP_SPECIAL_TOKEN[style]} {MAP_SPECIAL_TOKEN[sentiment]} {' '.join(keywords)}\""
      ],
      "metadata": {
        "id": "HQyU6q2oQdLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs = list(map(\n",
        "    lambda input_condition,output_sentence:(\n",
        "        format_condition(*input_condition),\n",
        "        format_sentence(output_sentence)\n",
        "    ),\n",
        "    *zip(*train_pairs)\n",
        "))"
      ],
      "metadata": {
        "id": "FASMVpBks74f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J760VvsStOAt",
        "outputId": "1fceda53-5c91-4e8c-e6f3-fbbf4cc2a6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<|sentiment:positive|> <|style:statement|> police crime',\n",
              "  '<|startoftext|> The police attended a crime </s>'),\n",
              " ('<|sentiment:negative|> <|style:statement|> hope verdict',\n",
              "  '<|startoftext|> Her hopes were dashed when she heard the verdict. </s>'),\n",
              " ('<|sentiment:neutral|> <|style:statement|> relax',\n",
              "  '<|startoftext|> Please relax. </s>'),\n",
              " ('<|sentiment:positive|> <|style:question|> japan food',\n",
              "  '<|startoftext|> I eat Japanese food. </s>'),\n",
              " ('<|sentiment:negative|> <|style:question|> mary present daughter',\n",
              "  \"<|startoftext|> Mary bought a present for her friend's daughter. </s>\")]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Encoding"
      ],
      "metadata": {
        "id": "3I_q_g1hIfot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.data import Dataset "
      ],
      "metadata": {
        "id": "YBGw1PTANdRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_vectorisation = lambda text: TOKENISER.encode(str(text),return_tensors='tf',padding='max_length',max_length=ModelSettings.SEQUENCE_LENGTH.value, truncation=True)\n",
        "output_vectorisation = lambda text: TOKENISER.encode(str(text),return_tensors='tf',padding='max_length',max_length=1+ModelSettings.SEQUENCE_LENGTH.value, truncation=True)"
      ],
      "metadata": {
        "id": "wKs2xr3A6aK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_data(input_condition:str, output_sentence:str) -> Tuple[Dict[str,str],str]:\n",
        "    encoder_input_ids = input_vectorisation(input_condition)\n",
        "    sentence_ids = output_vectorisation(output_sentence)\n",
        "    decoder_input_ids = sentence_ids[:,:-1]\n",
        "    decoder_output_ids = sentence_ids[:, 1:]\n",
        "    inputs = dict(\n",
        "      encoder_inputs= encoder_input_ids, \n",
        "      decoder_inputs= decoder_input_ids\n",
        "    )\n",
        "    return inputs, decoder_output_ids"
      ],
      "metadata": {
        "id": "BRSxFA42LfNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(pairs:List[Tuple[str,str]]) -> Dataset:\n",
        "    train_inputs,train_outputs = zip(*pairs)\n",
        "    dataset = Dataset.from_tensor_slices((list(train_inputs),list(train_outputs)))\n",
        "    dataset = dataset.batch(ModelSettings.BATCH_SIZE.value)\n",
        "    dataset = dataset.map(format_data)\n",
        "    return dataset.shuffle(ModelSettings.SHUFFLE_SEED.value).prefetch(ModelSettings.PREFETCH.value).cache()"
      ],
      "metadata": {
        "id": "SgznaO92Maue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = make_dataset(train_pairs)\n",
        "validation_dataset = make_dataset(train_pairs)"
      ],
      "metadata": {
        "id": "5PY2cMlGNKS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "D9jelYvvLatu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Layer, MultiHeadAttention, Dense, LayerNormalization, Embedding, Dropout\n",
        "from tensorflow import cast, newaxis, shape, range, minimum, reshape, concat, tile, expand_dims, constant\n",
        "from tensorflow.math import not_equal\n",
        "from keras import Sequential, Input, Model"
      ],
      "metadata": {
        "id": "IPdnk_He96hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.embed_dim = ModelSettings.EMBEDDING_DIMENSION.value\n",
        "        self.dense_dim = ModelSettings.LATENT_DIMENSION.value\n",
        "        self.num_heads = ModelSettings.NUMBER_HEADS.value\n",
        "        self.attention = MultiHeadAttention(\n",
        "            num_heads=ModelSettings.NUMBER_HEADS.value, \n",
        "            key_dim=ModelSettings.EMBEDDING_DIMENSION.value\n",
        "        )\n",
        "        self.dense_projection = Sequential([\n",
        "            Dense(ModelSettings.LATENT_DIMENSION.value, activation=\"relu\"), \n",
        "            Dense(ModelSettings.EMBEDDING_DIMENSION.value)\n",
        "        ])\n",
        "        self.layernorm_1 = LayerNormalization()\n",
        "        self.layernorm_2 = LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask):\n",
        "        attended_input = self.attention(\n",
        "            query=inputs, \n",
        "            value=inputs, \n",
        "            key=inputs, \n",
        "            attention_mask=cast(mask[:, newaxis, newaxis, :], dtype=\"int32\")\n",
        "        )\n",
        "        projected_input = self.layernorm_1(inputs + attended_input)\n",
        "        projected_output = self.dense_projection(projected_input)\n",
        "        return self.layernorm_2(projected_input + projected_output)"
      ],
      "metadata": {
        "id": "eAzi0KDZGXhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.token_embeddings = Embedding(\n",
        "            input_dim=len(TOKENISER), \n",
        "            output_dim=ModelSettings.EMBEDDING_DIMENSION.value\n",
        "        )\n",
        "        self.position_embeddings = Embedding(\n",
        "            input_dim=ModelSettings.SEQUENCE_LENGTH.value, \n",
        "            output_dim=ModelSettings.EMBEDDING_DIMENSION.value\n",
        "        )\n",
        "        self.sequence_length = ModelSettings.SEQUENCE_LENGTH.value\n",
        "        self.vocab_size = len(TOKENISER)\n",
        "        self.embed_dim = ModelSettings.LATENT_DIMENSION.value\n",
        "\n",
        "    def call(self, inputs):\n",
        "        positions = range(start=0, limit=shape(inputs)[-1], delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_mask(inputs, mask=None):\n",
        "        return not_equal(inputs, 0)"
      ],
      "metadata": {
        "id": "EuOhfBCj_qgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_dim = ModelSettings.EMBEDDING_DIMENSION.value\n",
        "        self.latent_dim = ModelSettings.LATENT_DIMENSION.value\n",
        "        self.num_heads = ModelSettings.NUMBER_HEADS.value\n",
        "        self.attention_1 = MultiHeadAttention(\n",
        "            num_heads=ModelSettings.NUMBER_HEADS.value, \n",
        "            key_dim=ModelSettings.EMBEDDING_DIMENSION.value\n",
        "        )\n",
        "        self.attention_2 = MultiHeadAttention(\n",
        "            num_heads=ModelSettings.NUMBER_HEADS.value, \n",
        "            key_dim=ModelSettings.EMBEDDING_DIMENSION.value\n",
        "        )\n",
        "        self.dense_projtion = Sequential([\n",
        "          Dense(ModelSettings.LATENT_DIMENSION.value, activation=ModelSettings.ACTIVATION.value), \n",
        "          Dense(ModelSettings.EMBEDDING_DIMENSION.value)\n",
        "        ])\n",
        "        self.layernorm_1 = LayerNormalization()\n",
        "        self.layernorm_2 = LayerNormalization()\n",
        "        self.layernorm_3 = LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = cast(mask[:, newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attended_inputs = self.attention_1(\n",
        "            query=inputs, \n",
        "            value=inputs, \n",
        "            key=inputs, \n",
        "            attention_mask=causal_mask\n",
        "        )\n",
        "        decoder_inputs = self.layernorm_1(inputs + attended_inputs)\n",
        "\n",
        "        attended_decoder_inputs = self.attention_2(\n",
        "            query=decoder_inputs,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        outputs = self.layernorm_2(decoder_inputs + attended_decoder_inputs)\n",
        "\n",
        "        projected_outputs = self.dense_projtion(outputs)\n",
        "        return self.layernorm_3(outputs + projected_outputs)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_causal_attention_mask(inputs):\n",
        "        input_shape = shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = range(sequence_length)[:, newaxis]\n",
        "        j = range(sequence_length)\n",
        "        mask = cast(i >= j, dtype=\"int32\")\n",
        "        mask = reshape(mask, (1, sequence_length, sequence_length))\n",
        "        multiple = concat([\n",
        "            expand_dims(batch_size, -1), \n",
        "            constant([1, 1], dtype=\"int32\")\n",
        "        ],axis=0)\n",
        "        return tile(mask, multiple)"
      ],
      "metadata": {
        "id": "U7uRhqHhGgcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import argmax, ndarray\n",
        "\n",
        "class ConditionalGeneratorTransformer:\n",
        "  def __init__(self) -> None:\n",
        "      self.model = self._build_model()\n",
        "  \n",
        "  def train(self,data:Dataset, validation_data:Dataset) -> None:\n",
        "      self.model.fit(data, epochs=ModelSettings.EPOCHS.value, validation_data=validation_data)\n",
        "\n",
        "  def generate(self, style:str, sentiment:str, keywords:List[str]) -> str:\n",
        "      input_condition = format_condition(style,sentiment,keywords)\n",
        "      tokenised_input_condition = input_vectorisation([input_condition])\n",
        "      decoded_sentence = TOKENISER.bos_token\n",
        "\n",
        "      for position in range(ModelSettings.MAXIMUM_GENERATION_LENGTH.value):\n",
        "          tokenised_target_sentence = output_vectorisation([decoded_sentence])[:, :-1]\n",
        "\n",
        "          logits = self.model([tokenised_input_condition, tokenised_target_sentence])\n",
        "\n",
        "          predicted_token_id = self._greedy_decode(logits, position)\n",
        "          predicted_token = TOKENISER.decode(predicted_token_id)\n",
        "          if predicted_token == TOKENISER.eos_token:\n",
        "              break\n",
        "          decoded_sentence += f\" {predicted_token}\"\n",
        "\n",
        "      return decoded_sentence\n",
        "  \n",
        "  @staticmethod\n",
        "  def _greedy_decode(logits:ndarray,position:int) -> int:\n",
        "    return argmax(logits[0, position, :])          \n",
        "\n",
        "  @staticmethod\n",
        "  def _build_model() -> Model:\n",
        "      encoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "      contextualised_encoder_inputs = PositionalEmbedding()(encoder_inputs)\n",
        "      encoder_outputs = TransformerEncoder()(contextualised_encoder_inputs)\n",
        "      encoder = Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "      decoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "      encoded_inputs = Input(shape=(None, ModelSettings.EMBEDDING_DIMENSION.value), name=\"decoder_state_inputs\")\n",
        "      contextualised_decoder_inputs = PositionalEmbedding()(decoder_inputs)\n",
        "      projected_decoder_inputs = TransformerDecoder()(contextualised_decoder_inputs, encoded_inputs)\n",
        "      decoder_logits = Dropout(ModelSettings.DROPOUT_RATE.value)(projected_decoder_inputs)\n",
        "      decoder_outputs = Dense(len(TOKENISER), activation=\"softmax\")(decoder_logits)\n",
        "      decoder = Model([decoder_inputs, encoded_inputs], decoder_outputs)\n",
        "\n",
        "      transformer_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "\n",
        "      transformer = Model(\n",
        "          [encoder_inputs, decoder_inputs], \n",
        "          transformer_outputs, \n",
        "          name=\"transformer\"\n",
        "      )\n",
        "      transformer.compile(ModelSettings.OPTIMISER.value, loss=ModelSettings.LOSS.value, metrics=[ModelSettings.METRIC.value])\n",
        "      return transformer"
      ],
      "metadata": {
        "id": "GKEbNyQYGdCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "3uX8a8-5Mxws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = ConditionalGeneratorTransformer()\n",
        "x.train(train_dataset, validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVQBMzwxMrVu",
        "outputId": "ce97e7d9-1c98-425e-c712-bfba7fb05bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 5s 5s/step - loss: 10.4504 - accuracy: 0.0000e+00 - val_loss: 9.5574 - val_accuracy: 0.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "HpfSmK-9M6G-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = [\"car\",\"made\"]"
      ],
      "metadata": {
        "id": "laoLNboSNI4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.generate(\"statement\",\"neutral\",keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BZcciNQEM5rX",
        "outputId": "c304eabc-0604-4c20-ee92-6adace428b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|startoftext|> s s ( s _ s _ ( s _ s : = ( s one  s '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.generate(\"statement\",\"negative\",keywords)"
      ],
      "metadata": {
        "id": "jHdJ8WBzNG_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.generate(\"statement\",\"positive\",keywords)"
      ],
      "metadata": {
        "id": "MMKShExqNM5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.generate(\"question\",\"neutral\",keywords)"
      ],
      "metadata": {
        "id": "dkM4dRQGNPtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.generate(\"question\",\"negative\",keywords)"
      ],
      "metadata": {
        "id": "YdFJ6wwdNSJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.generate(\"question\",\"positive\",keywords)"
      ],
      "metadata": {
        "id": "-tATnRAaNUKP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}